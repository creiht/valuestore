package valuestore

import (
    "bytes"
    "encoding/binary"
    "errors"
    "fmt"
    "io"
    "math"
    "math/rand"
    "os"
    "path"
    "sort"
    "strconv"
    "strings"
    "sync"
    "sync/atomic"
    "time"

    "github.com/gholt/ring"
    "github.com/gholt/valuelocmap"
    "github.com/spaolacci/murmur3"
    "gopkg.in/gholt/brimutil.v1"
)

// {{.T}}Store is an interface for a disk-backed data structure that stores
// []byte values referenced by 128 bit keys with options for replication.
//
// For documentation on each of these functions, see the Default{{.T}}Store.
type {{.T}}Store interface {
    Lookup(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}) (int64, uint32, error)
    Read(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, value []byte) (int64, []byte, error)
    {{if eq .t "group"}}
    ReadGroup(keyA uint64, keyB uint64) (int, chan *ReadGroupItem)
    {{end}}
    Write(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, timestamp int64, value []byte) (int64, error)
    Delete(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, timestamp int64) (int64, error)
    EnableAll()
    DisableAll()
    DisableAllBackground()
    EnableTombstoneDiscard()
    DisableTombstoneDiscard()
    TombstoneDiscardPass()
    EnableCompaction()
    DisableCompaction()
    CompactionPass()
    EnableOutPullReplication()
    DisableOutPullReplication()
    OutPullReplicationPass()
    EnableOutPushReplication()
    DisableOutPushReplication()
    OutPushReplicationPass()
    EnableWrites()
    DisableWrites()
    Flush()
    Stats(debug bool) fmt.Stringer
    ValueCap() uint32
}

// Default{{.T}}Store instances are created with New{{.T}}Store.
type Default{{.T}}Store struct {
    logCritical             LogFunc
    logError                LogFunc
    logWarning              LogFunc
    logInfo                 LogFunc
    logDebug                LogFunc
    randMutex               sync.Mutex
    rand                    *rand.Rand
    freeableVMChans         []chan *{{.t}}Mem
    freeVMChan              chan *{{.t}}Mem
    freeVWRChans            []chan *{{.t}}WriteReq
    pendingVWRChans         []chan *{{.t}}WriteReq
    vfVMChan                chan *{{.t}}Mem
    freeTOCBlockChan        chan []byte
    pendingTOCBlockChan     chan []byte
    activeTOCA              uint64
    activeTOCB              uint64
    flushedChan             chan struct{}
    locBlocks               []{{.t}}LocBlock
    locBlockIDer            uint64
    path                    string
    pathtoc                 string
    vlm                     valuelocmap.{{.T}}LocMap
    workers                 int
    recoveryBatchSize       int
    valueCap                uint32
    pageSize                uint32
    minValueAlloc           int
    writePagesPerWorker     int
    fileCap                 uint32
    fileReaders             int
    checksumInterval        uint32
    msgRing                 ring.MsgRing
    tombstoneDiscardState   {{.t}}TombstoneDiscardState
    replicationIgnoreRecent uint64
    pullReplicationState    {{.t}}PullReplicationState
    pushReplicationState    {{.t}}PushReplicationState
    compactionState         {{.t}}CompactionState
    bulkSetState            {{.t}}BulkSetState
    bulkSetAckState         {{.t}}BulkSetAckState
    disableEnableWritesLock sync.Mutex
    userDisabled            bool
    diskWatcherState        {{.t}}DiskWatcherState

    statsLock                    sync.Mutex
    lookups                      int32
    lookupErrors                 int32
    reads                        int32
    readErrors                   int32
    readGroups                   int32
    readGroupItems               int32
    writes                       int32
    writeErrors                  int32
    writesOverridden             int32
    deletes                      int32
    deleteErrors                 int32
    deletesOverridden            int32
    outBulkSets                  int32
    outBulkSetValues             int32
    outBulkSetPushes             int32
    outBulkSetPushValues         int32
    inBulkSets                   int32
    inBulkSetDrops               int32
    inBulkSetInvalids            int32
    inBulkSetWrites              int32
    inBulkSetWriteErrors         int32
    inBulkSetWritesOverridden    int32
    outBulkSetAcks               int32
    inBulkSetAcks                int32
    inBulkSetAckDrops            int32
    inBulkSetAckInvalids         int32
    inBulkSetAckWrites           int32
    inBulkSetAckWriteErrors      int32
    inBulkSetAckWritesOverridden int32
    outPullReplications          int32
    inPullReplications           int32
    inPullReplicationDrops       int32
    inPullReplicationInvalids    int32
    expiredDeletions             int32
    compactions                  int32
    smallFileCompactions         int32
}

type {{.t}}WriteReq struct {
    keyA          uint64
    keyB          uint64
    {{if eq .t "group"}}
    nameKeyA      uint64
    nameKeyB      uint64
    {{end}}
    timestampbits uint64
    value         []byte
    errChan       chan error
    internal      bool
}

var enable{{.T}}WriteReq *{{.t}}WriteReq = &{{.t}}WriteReq{}
var disable{{.T}}WriteReq *{{.t}}WriteReq = &{{.t}}WriteReq{}
var flush{{.T}}WriteReq *{{.t}}WriteReq = &{{.t}}WriteReq{}
var flush{{.T}}Mem *{{.t}}Mem = &{{.t}}Mem{}

type {{.t}}LocBlock interface {
    timestampnano() int64
    read(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, timestampbits uint64, offset uint32, length uint32, value []byte) (uint64, []byte, error)
    close() error
}

// New{{.T}}Store creates a Default{{.T}}Store for use in storing []byte values
// referenced by 128 bit keys.
//
// Note that a lot of buffering, multiple cores, and background processes can
// be in use and therefore DisableAll() and Flush() should be called prior to
// the process exiting to ensure all processing is done and the buffers are
// flushed.
func New{{.T}}Store(c *{{.T}}StoreConfig) (*Default{{.T}}Store, error) {
    cfg := resolve{{.T}}StoreConfig(c)
    vlm := cfg.{{.T}}LocMap
    if vlm == nil {
        vlm = valuelocmap.New{{.T}}LocMap(nil)
    }
    vlm.SetInactiveMask(_TSB_INACTIVE)
    vs := &Default{{.T}}Store{
        logCritical:             cfg.LogCritical,
        logError:                cfg.LogError,
        logWarning:              cfg.LogWarning,
        logInfo:                 cfg.LogInfo,
        logDebug:                cfg.LogDebug,
        rand:                    cfg.Rand,
        locBlocks:          make([]{{.t}}LocBlock, math.MaxUint16),
        path:                    cfg.Path,
        pathtoc:                 cfg.PathTOC,
        vlm:                     vlm,
        workers:                 cfg.Workers,
        recoveryBatchSize:       cfg.RecoveryBatchSize,
        replicationIgnoreRecent: (uint64(cfg.ReplicationIgnoreRecent) * uint64(time.Second) / 1000) << _TSB_UTIL_BITS,
        valueCap:                uint32(cfg.ValueCap),
        pageSize:                uint32(cfg.PageSize),
        minValueAlloc:           cfg.minValueAlloc,
        writePagesPerWorker:     cfg.WritePagesPerWorker,
        fileCap:           uint32(cfg.FileCap),
        fileReaders:       cfg.FileReaders,
        checksumInterval:        uint32(cfg.ChecksumInterval),
        msgRing:                 cfg.MsgRing,
    }
    vs.freeableVMChans = make([]chan *{{.t}}Mem, vs.workers)
    for i := 0; i < cap(vs.freeableVMChans); i++ {
        vs.freeableVMChans[i] = make(chan *{{.t}}Mem, vs.workers)
    }
    vs.freeVMChan = make(chan *{{.t}}Mem, vs.workers*vs.writePagesPerWorker)
    vs.freeVWRChans = make([]chan *{{.t}}WriteReq, vs.workers)
    vs.pendingVWRChans = make([]chan *{{.t}}WriteReq, vs.workers)
    vs.vfVMChan = make(chan *{{.t}}Mem, vs.workers)
    vs.freeTOCBlockChan = make(chan []byte, vs.workers*2)
    vs.pendingTOCBlockChan = make(chan []byte, vs.workers)
    vs.flushedChan = make(chan struct{}, 1)
    for i := 0; i < cap(vs.freeVMChan); i++ {
        vm := &{{.t}}Mem{
            vs:     vs,
            toc:    make([]byte, 0, vs.pageSize),
            values: make([]byte, 0, vs.pageSize),
        }
        var err error
        vm.id, err = vs.addLocBlock(vm)
        if err != nil {
            return nil, err
        }
        vs.freeVMChan <- vm
    }
    for i := 0; i < len(vs.freeVWRChans); i++ {
        vs.freeVWRChans[i] = make(chan *{{.t}}WriteReq, vs.workers*2)
        for j := 0; j < vs.workers*2; j++ {
            vs.freeVWRChans[i] <- &{{.t}}WriteReq{errChan: make(chan error, 1)}
        }
    }
    for i := 0; i < len(vs.pendingVWRChans); i++ {
        vs.pendingVWRChans[i] = make(chan *{{.t}}WriteReq)
    }
    for i := 0; i < cap(vs.freeTOCBlockChan); i++ {
        vs.freeTOCBlockChan <- make([]byte, 0, vs.pageSize)
    }
    go vs.tocWriter()
    go vs.vfWriter()
    for i := 0; i < len(vs.freeableVMChans); i++ {
        go vs.memClearer(vs.freeableVMChans[i])
    }
    for i := 0; i < len(vs.pendingVWRChans); i++ {
        go vs.memWriter(vs.pendingVWRChans[i])
    }
    err := vs.recovery()
    if err != nil {
        return nil, err
    }
    vs.tombstoneDiscardConfig(cfg)
    vs.compactionConfig(cfg)
    vs.pullReplicationConfig(cfg)
    vs.pushReplicationConfig(cfg)
    vs.bulkSetConfig(cfg)
    vs.bulkSetAckConfig(cfg)
    vs.diskWatcherConfig(cfg)
    vs.tombstoneDiscardLaunch()
    vs.compactionLaunch()
    vs.pullReplicationLaunch()
    vs.pushReplicationLaunch()
    vs.bulkSetLaunch()
    vs.bulkSetAckLaunch()
    vs.diskWatcherLaunch()
    return vs, nil
}

// ValueCap returns the maximum length of a value the {{.T}}Store can accept.
func (vs *Default{{.T}}Store) ValueCap() uint32 {
    return vs.valueCap
}

// DisableAll calls DisableAllBackground(), and DisableWrites().
func (vs *Default{{.T}}Store) DisableAll() {
    vs.DisableAllBackground()
    vs.DisableWrites()
}

// DisableAllBackground calls DisableTombstoneDiscard(), DisableCompaction(),
// DisableOutPullReplication(), DisableOutPushReplication(), but does *not*
// call DisableWrites().
func (vs *Default{{.T}}Store) DisableAllBackground() {
    vs.DisableTombstoneDiscard()
    vs.DisableCompaction()
    vs.DisableOutPullReplication()
    vs.DisableOutPushReplication()
}

// EnableAll calls EnableTombstoneDiscard(), EnableCompaction(),
// EnableOutPullReplication(), EnableOutPushReplication(), and EnableWrites().
func (vs *Default{{.T}}Store) EnableAll() {
    vs.EnableTombstoneDiscard()
    vs.EnableOutPullReplication()
    vs.EnableOutPushReplication()
    vs.EnableWrites()
    vs.EnableCompaction()
}

// DisableWrites will cause any incoming Write or Delete requests to respond
// with ErrDisabled until EnableWrites is called.
func (vs *Default{{.T}}Store) DisableWrites() {
    vs.disableWrites(true)
}

func (vs *Default{{.T}}Store) disableWrites(userCall bool) {
    vs.disableEnableWritesLock.Lock()
    if userCall {
        vs.userDisabled = true
    }
    for _, c := range vs.pendingVWRChans {
        c <- disable{{.T}}WriteReq
    }
    vs.disableEnableWritesLock.Unlock()
}

// EnableWrites will resume accepting incoming Write and Delete requests.
func (vs *Default{{.T}}Store) EnableWrites() {
    vs.enableWrites(true)
}

func (vs *Default{{.T}}Store) enableWrites(userCall bool) {
    vs.disableEnableWritesLock.Lock()
    if userCall || !vs.userDisabled {
        vs.userDisabled = false
        for _, c := range vs.pendingVWRChans {
            c <- enable{{.T}}WriteReq
        }
    }
    vs.disableEnableWritesLock.Unlock()
}

// Flush will ensure buffered data (at the time of the call) is written to
// disk.
func (vs *Default{{.T}}Store) Flush() {
    for _, c := range vs.pendingVWRChans {
        c <- flush{{.T}}WriteReq
    }
    <-vs.flushedChan
}

// Lookup will return timestampmicro, length, err for keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}.
//
// Note that err == ErrNotFound with timestampmicro == 0 indicates keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}
// was not known at all whereas err == ErrNotFound with timestampmicro != 0
// indicates keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}
// was known and had a deletion marker (aka tombstone).
func (vs *Default{{.T}}Store) Lookup(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}) (int64, uint32, error) {
    atomic.AddInt32(&vs.lookups, 1)
    timestampbits, _, length, err := vs.lookup(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}})
    if err != nil {
        atomic.AddInt32(&vs.lookupErrors, 1)
    }
    return int64(timestampbits >> _TSB_UTIL_BITS), length, err
}

func (vs *Default{{.T}}Store) lookup(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}) (uint64, uint32, uint32, error) {
    timestampbits, id, _, length := vs.vlm.Get(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}})
    if id == 0 || timestampbits&_TSB_DELETION != 0 {
        return timestampbits, id, 0, ErrNotFound
    }
    return timestampbits, id, length, nil
}

// Read will return timestampmicro, value, err for keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}};
// if an incoming value is provided, the read value will be appended to it and
// the whole returned (useful to reuse an existing []byte).
//
// Note that err == ErrNotFound with timestampmicro == 0 indicates keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}
// was not known at all whereas err == ErrNotFound with timestampmicro != 0
// indicates keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}} was known and had a deletion marker (aka tombstone).
func (vs *Default{{.T}}Store) Read(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, value []byte) (int64, []byte, error) {
    atomic.AddInt32(&vs.reads, 1)
    timestampbits, value, err := vs.read(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}, value)
    if err != nil {
        atomic.AddInt32(&vs.readErrors, 1)
    }
    return int64(timestampbits >> _TSB_UTIL_BITS), value, err
}

func (vs *Default{{.T}}Store) read(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, value []byte) (uint64, []byte, error) {
    timestampbits, id, offset, length := vs.vlm.Get(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}})
    if id == 0 || timestampbits&_TSB_DELETION != 0 || timestampbits&_TSB_LOCAL_REMOVAL != 0 {
        return timestampbits, value, ErrNotFound
    }
    return vs.locBlock(id).read(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}, timestampbits, offset, length, value)
}

{{if eq .t "group"}}
type ReadGroupItem struct {
    Error           error
    NameKeyA        uint64
    NameKeyB        uint64
    TimestampMicro  uint64
    Value           []byte
}

func (vs *Default{{.T}}Store) ReadGroup(keyA uint64, keyB uint64) (int, chan *ReadGroupItem) {
    atomic.AddInt32(&vs.readGroups, 1)
    items := vs.vlm.GetGroup(keyA, keyB)
    c := make(chan *ReadGroupItem, vs.workers)
    if len(items) == 0 {
        close(c)
        return 0, c
    }
    atomic.AddInt32(&vs.readGroupItems, int32(len(items)))
    go func() {
        for _, item := range items {
            t, v, err := vs.read(keyA, keyB, item.NameKeyA, item.NameKeyB, nil)
            if err != nil {
                if err != ErrNotFound {
                    c <- &ReadGroupItem{Error: err}
                    break
                }
            }
            c <- &ReadGroupItem{
                NameKeyA:       item.NameKeyA,
                NameKeyB:       item.NameKeyB,
                TimestampMicro: t,
                Value:          v,
            }
        }
        close(c)
    }()
    return len(items), c
}
{{end}}

// Write stores timestampmicro, value for keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}
// and returns the previously stored timestampmicro or returns any error; a
// newer timestampmicro already in place is not reported as an error. Note that
// with a write and a delete for the exact same timestampmicro, the delete
// wins.
func (vs *Default{{.T}}Store) Write(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, timestampmicro int64, value []byte) (int64, error) {
    atomic.AddInt32(&vs.writes, 1)
    if timestampmicro < TIMESTAMPMICRO_MIN {
        atomic.AddInt32(&vs.writeErrors, 1)
        return 0, fmt.Errorf("timestamp %d < %d", timestampmicro, TIMESTAMPMICRO_MIN)
    }
    if timestampmicro > TIMESTAMPMICRO_MAX {
        atomic.AddInt32(&vs.writeErrors, 1)
        return 0, fmt.Errorf("timestamp %d > %d", timestampmicro, TIMESTAMPMICRO_MAX)
    }
    timestampbits, err := vs.write(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}, uint64(timestampmicro)<<_TSB_UTIL_BITS, value, false)
    if err != nil {
        atomic.AddInt32(&vs.writeErrors, 1)
    }
    if timestampmicro <= int64(timestampbits>>_TSB_UTIL_BITS) {
        atomic.AddInt32(&vs.writesOverridden, 1)
    }
    return int64(timestampbits >> _TSB_UTIL_BITS), err
}

func (vs *Default{{.T}}Store) write(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, timestampbits uint64, value []byte, internal bool) (uint64, error) {
    i := int(keyA>>1) % len(vs.freeVWRChans)
    vwr := <-vs.freeVWRChans[i]
    vwr.keyA = keyA
    vwr.keyB = keyB
    {{if eq .t "group"}}
    vwr.nameKeyA = nameKeyA
    vwr.nameKeyB = nameKeyB
    {{end}}
    vwr.timestampbits = timestampbits
    vwr.value = value
    vwr.internal = internal
    vs.pendingVWRChans[i] <- vwr
    err := <-vwr.errChan
    ptimestampbits := vwr.timestampbits
    vwr.value = nil
    vs.freeVWRChans[i] <- vwr
    return ptimestampbits, err
}

// Delete stores timestampmicro for keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}
// and returns the previously stored timestampmicro or returns any error; a
// newer timestampmicro already in place is not reported as an error. Note that
// with a write and a delete for the exact same timestampmicro, the delete
// wins.
func (vs *Default{{.T}}Store) Delete(keyA uint64, keyB uint64{{if eq .t "group"}}, nameKeyA uint64, nameKeyB uint64{{end}}, timestampmicro int64) (int64, error) {
    atomic.AddInt32(&vs.deletes, 1)
    if timestampmicro < TIMESTAMPMICRO_MIN {
        atomic.AddInt32(&vs.deleteErrors, 1)
        return 0, fmt.Errorf("timestamp %d < %d", timestampmicro, TIMESTAMPMICRO_MIN)
    }
    if timestampmicro > TIMESTAMPMICRO_MAX {
        atomic.AddInt32(&vs.deleteErrors, 1)
        return 0, fmt.Errorf("timestamp %d > %d", timestampmicro, TIMESTAMPMICRO_MAX)
    }
    ptimestampbits, err := vs.write(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}, (uint64(timestampmicro)<<_TSB_UTIL_BITS)|_TSB_DELETION, nil, true)
    if err != nil {
        atomic.AddInt32(&vs.deleteErrors, 1)
    }
    if timestampmicro <= int64(ptimestampbits>>_TSB_UTIL_BITS) {
        atomic.AddInt32(&vs.deletesOverridden, 1)
    }
    return int64(ptimestampbits >> _TSB_UTIL_BITS), err
}

func (vs *Default{{.T}}Store) locBlock(locBlockID uint32) {{.t}}LocBlock {
    return vs.locBlocks[locBlockID]
}

func (vs *Default{{.T}}Store) addLocBlock(block {{.t}}LocBlock) (uint32, error) {
    id := atomic.AddUint64(&vs.locBlockIDer, 1)
    if id >= math.MaxUint32 {
        return 0, errors.New("too many loc blocks")
    }
    vs.locBlocks[id] = block
    return uint32(id), nil
}

func (vs *Default{{.T}}Store) locBlockIDFromTimestampnano(tsn int64) uint32 {
    for i := 1; i <= len(vs.locBlocks); i++ {
        if vs.locBlocks[i] == nil {
            return 0
        } else {
            if tsn == vs.locBlocks[i].timestampnano() {
                return uint32(i)
            }
        }
    }
    return 0
}

func (vs *Default{{.T}}Store) closeLocBlock(locBlockID uint32) error {
    return vs.locBlocks[locBlockID].close()
}

func (vs *Default{{.T}}Store) memClearer(freeableVMChan chan *{{.t}}Mem) {
    var tb []byte
    var tbTS int64
    var tbOffset int
    for {
        vm := <-freeableVMChan
        if vm == flush{{.T}}Mem {
            if tb != nil {
                vs.pendingTOCBlockChan <- tb
                tb = nil
            }
            vs.pendingTOCBlockChan <- nil
            continue
        }
        vf := vs.locBlock(vm.vfID)
        if tb != nil && tbTS != vf.timestampnano() {
            vs.pendingTOCBlockChan <- tb
            tb = nil
        }
        for vmTOCOffset := 0; vmTOCOffset < len(vm.toc); vmTOCOffset += _{{.TT}}_FILE_ENTRY_SIZE {
            {{if eq .t "value"}}
            keyA := binary.BigEndian.Uint64(vm.toc[vmTOCOffset:])
            keyB := binary.BigEndian.Uint64(vm.toc[vmTOCOffset+8:])
            timestampbits := binary.BigEndian.Uint64(vm.toc[vmTOCOffset+16:])
            {{else}}
            keyA := binary.BigEndian.Uint64(vm.toc[vmTOCOffset:])
            keyB := binary.BigEndian.Uint64(vm.toc[vmTOCOffset+8:])
            nameKeyA := binary.BigEndian.Uint64(vm.toc[vmTOCOffset+16:])
            nameKeyB := binary.BigEndian.Uint64(vm.toc[vmTOCOffset+24:])
            timestampbits := binary.BigEndian.Uint64(vm.toc[vmTOCOffset+32:])
            {{end}}
            var blockID uint32
            var offset uint32
            var length uint32
            if timestampbits&_TSB_LOCAL_REMOVAL == 0 {
                blockID = vm.vfID
                offset = vm.vfOffset + binary.BigEndian.Uint32(vm.toc[vmTOCOffset+24:])
                length = binary.BigEndian.Uint32(vm.toc[vmTOCOffset+28:])
            }
            if vs.vlm.Set(keyA, keyB{{if eq .t "group"}}, nameKeyA, nameKeyB{{end}}, timestampbits, blockID, offset, length, true) > timestampbits {
                continue
            }
            if tb != nil && tbOffset+_{{.TT}}_FILE_ENTRY_SIZE > cap(tb) {
                vs.pendingTOCBlockChan <- tb
                tb = nil
            }
            if tb == nil {
                tb = <-vs.freeTOCBlockChan
                tbTS = vf.timestampnano()
                tb = tb[:8]
                binary.BigEndian.PutUint64(tb, uint64(tbTS))
                tbOffset = 8
            }
            tb = tb[:tbOffset+_{{.TT}}_FILE_ENTRY_SIZE]
            binary.BigEndian.PutUint64(tb[tbOffset:], keyA)
            binary.BigEndian.PutUint64(tb[tbOffset+8:], keyB)
            binary.BigEndian.PutUint64(tb[tbOffset+16:], timestampbits)
            binary.BigEndian.PutUint32(tb[tbOffset+24:], offset)
            binary.BigEndian.PutUint32(tb[tbOffset+28:], length)
            tbOffset += _{{.TT}}_FILE_ENTRY_SIZE
        }
        vm.discardLock.Lock()
        vm.vfID = 0
        vm.vfOffset = 0
        vm.toc = vm.toc[:0]
        vm.values = vm.values[:0]
        vm.discardLock.Unlock()
        vs.freeVMChan <- vm
    }
}

func (vs *Default{{.T}}Store) memWriter(pendingVWRChan chan *{{.t}}WriteReq) {
    var enabled bool
    var vm *{{.t}}Mem
    var vmTOCOffset int
    var vmMemOffset int
    for {
        vwr := <-pendingVWRChan
        if vwr == enable{{.T}}WriteReq {
            enabled = true
            continue
        }
        if vwr == disable{{.T}}WriteReq {
            enabled = false
            continue
        }
        if vwr == flush{{.T}}WriteReq {
            if vm != nil && len(vm.toc) > 0 {
                vs.vfVMChan <- vm
                vm = nil
            }
            vs.vfVMChan <- flush{{.T}}Mem
            continue
        }
        if !enabled && !vwr.internal {
            vwr.errChan <- ErrDisabled
            continue
        }
        length := len(vwr.value)
        if length > int(vs.valueCap) {
            vwr.errChan <- fmt.Errorf("value length of %d > %d", length, vs.valueCap)
            continue
        }
        alloc := length
        if alloc < vs.minValueAlloc {
            alloc = vs.minValueAlloc
        }
        if vm != nil && (vmTOCOffset+_{{.TT}}_FILE_ENTRY_SIZE > cap(vm.toc) || vmMemOffset+alloc > cap(vm.values)) {
            vs.vfVMChan <- vm
            vm = nil
        }
        if vm == nil {
            vm = <-vs.freeVMChan
            vmTOCOffset = 0
            vmMemOffset = 0
        }
        vm.discardLock.Lock()
        vm.values = vm.values[:vmMemOffset+alloc]
        vm.discardLock.Unlock()
        copy(vm.values[vmMemOffset:], vwr.value)
        if alloc > length {
            for i, j := vmMemOffset+length, vmMemOffset+alloc; i < j; i++ {
                vm.values[i] = 0
            }
        }
        ptimestampbits := vs.vlm.Set(vwr.keyA, vwr.keyB{{if eq .t "group"}}, vwr.nameKeyA, vwr.nameKeyB{{end}}, vwr.timestampbits, vm.id, uint32(vmMemOffset), uint32(length), false)
        if ptimestampbits < vwr.timestampbits {
            vm.toc = vm.toc[:vmTOCOffset+_{{.TT}}_FILE_ENTRY_SIZE]
            {{if eq .t "value"}}
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset:], vwr.keyA)
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset+8:], vwr.keyB)
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset+16:], vwr.timestampbits)
            binary.BigEndian.PutUint32(vm.toc[vmTOCOffset+24:], uint32(vmMemOffset))
            binary.BigEndian.PutUint32(vm.toc[vmTOCOffset+28:], uint32(length))
            {{else}}
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset:], vwr.keyA)
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset+8:], vwr.keyB)
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset+16:], vwr.nameKeyA)
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset+24:], vwr.nameKeyB)
            binary.BigEndian.PutUint64(vm.toc[vmTOCOffset+32:], vwr.timestampbits)
            binary.BigEndian.PutUint32(vm.toc[vmTOCOffset+40:], uint32(vmMemOffset))
            binary.BigEndian.PutUint32(vm.toc[vmTOCOffset+44:], uint32(length))
            {{end}}
            vmTOCOffset += _{{.TT}}_FILE_ENTRY_SIZE
            vmMemOffset += alloc
        } else {
            vm.discardLock.Lock()
            vm.values = vm.values[:vmMemOffset]
            vm.discardLock.Unlock()
        }
        vwr.timestampbits = ptimestampbits
        vwr.errChan <- nil
    }
}

func (vs *Default{{.T}}Store) vfWriter() {
    var vf *{{.t}}File
    memWritersFlushLeft := len(vs.pendingVWRChans)
    var tocLen uint64
    var valueLen uint64
    for {
        vm := <-vs.vfVMChan
        if vm == flush{{.T}}Mem {
            memWritersFlushLeft--
            if memWritersFlushLeft > 0 {
                continue
            }
            if vf != nil {
                err := vf.close()
                if err != nil {
                    vs.logCritical("error closing %s: %s\n", vf.name, err)
                }
                vf = nil
            }
            for i := 0; i < len(vs.freeableVMChans); i++ {
                vs.freeableVMChans[i] <- flush{{.T}}Mem
            }
            memWritersFlushLeft = len(vs.pendingVWRChans)
            continue
        }
        if vf != nil && (tocLen+uint64(len(vm.toc)) >= uint64(vs.fileCap) || valueLen+uint64(len(vm.values)) > uint64(vs.fileCap)) {
            err := vf.close()
            if err != nil {
                vs.logCritical("error closing %s: %s\n", vf.name, err)
            }
            vf = nil
        }
        if vf == nil {
            var err error
            vf, err = create{{.T}}File(vs, osCreateWriteCloser, osOpenReadSeeker)
            if err != nil {
                vs.logCritical("vfWriter: %s\n", err)
                break
            }
            tocLen = _{{.TT}}_FILE_HEADER_SIZE
            valueLen = _{{.TT}}_FILE_HEADER_SIZE
        }
        vf.write(vm)
        tocLen += uint64(len(vm.toc))
        valueLen += uint64(len(vm.values))
    }
}

func (vs *Default{{.T}}Store) tocWriter() {
    // writerA is the current toc file while writerB is the previously active
    // toc writerB is kept around in case a "late" key arrives to be flushed
    // whom's value is actually in the previous value file.
    memClearersFlushLeft := len(vs.freeableVMChans)
    var writerA io.WriteCloser
    var offsetA uint64
    var writerB io.WriteCloser
    var offsetB uint64
    var err error
    head := []byte("{{.TT}}STORETOC v0                ")
    binary.BigEndian.PutUint32(head[28:], uint32(vs.checksumInterval))
    term := make([]byte, 16)
    copy(term[12:], "TERM")
OuterLoop:
    for {
        t := <-vs.pendingTOCBlockChan
        if t == nil {
            memClearersFlushLeft--
            if memClearersFlushLeft > 0 {
                continue
            }
            if writerB != nil {
                binary.BigEndian.PutUint64(term[4:], offsetB)
                if _, err = writerB.Write(term); err != nil {
                    break OuterLoop
                }
                if err = writerB.Close(); err != nil {
                    break OuterLoop
                }
                writerB = nil
                atomic.StoreUint64(&vs.activeTOCB, 0)
                offsetB = 0
            }
            if writerA != nil {
                binary.BigEndian.PutUint64(term[4:], offsetA)
                if _, err = writerA.Write(term); err != nil {
                    break OuterLoop
                }
                if err = writerA.Close(); err != nil {
                    break OuterLoop
                }
                writerA = nil
                atomic.StoreUint64(&vs.activeTOCA, 0)
                offsetA = 0
            }
            vs.flushedChan <- struct{}{}
            memClearersFlushLeft = len(vs.freeableVMChans)
            continue
        }
        if len(t) > 8 {
            bts := binary.BigEndian.Uint64(t)
            switch bts {
            case atomic.LoadUint64(&vs.activeTOCA):
                if _, err = writerA.Write(t[8:]); err != nil {
                    break OuterLoop
                }
                offsetA += uint64(len(t) - 8)
            case atomic.LoadUint64(&vs.activeTOCB):
                if _, err = writerB.Write(t[8:]); err != nil {
                    break OuterLoop
                }
                offsetB += uint64(len(t) - 8)
            default:
                // An assumption is made here: If the timestampnano for this
                // toc block doesn't match the last two seen timestampnanos
                // then we expect no more toc blocks for the oldest
                // timestampnano and can close that toc file.
                if writerB != nil {
                    binary.BigEndian.PutUint64(term[4:], offsetB)
                    if _, err = writerB.Write(term); err != nil {
                        break OuterLoop
                    }
                    if err = writerB.Close(); err != nil {
                        break OuterLoop
                    }
                }
                atomic.StoreUint64(&vs.activeTOCB, atomic.LoadUint64(&vs.activeTOCA))
                writerB = writerA
                offsetB = offsetA
                atomic.StoreUint64(&vs.activeTOCA, bts)
                var fp *os.File
                fp, err = os.Create(path.Join(vs.pathtoc, fmt.Sprintf("%d.{{.t}}toc", bts)))
                if err != nil {
                    break OuterLoop
                }
                writerA = brimutil.NewMultiCoreChecksummedWriter(fp, int(vs.checksumInterval), murmur3.New32, vs.workers)
                if _, err = writerA.Write(head); err != nil {
                    break OuterLoop
                }
                if _, err = writerA.Write(t[8:]); err != nil {
                    break OuterLoop
                }
                offsetA = _{{.TT}}_FILE_HEADER_SIZE + uint64(len(t)-8)
            }
        }
        vs.freeTOCBlockChan <- t[:0]
    }
    if err != nil {
        vs.logCritical("tocWriter: %s\n", err)
    }
    if writerA != nil {
        writerA.Close()
    }
    if writerB != nil {
        writerB.Close()
    }
}

func (vs *Default{{.T}}Store) recovery() error {
    start := time.Now()
    fromDiskCount := 0
    causedChangeCount := int64(0)
    type writeReq struct {
        keyA          uint64
        keyB          uint64
        {{if eq .t "group"}}
        nameKeyA      uint64
        nameKeyB      uint64
        {{end}}
        timestampbits uint64
        blockID       uint32
        offset        uint32
        length        uint32
    }
    workers := uint64(vs.workers)
    pendingBatchChans := make([]chan []writeReq, workers)
    freeBatchChans := make([]chan []writeReq, len(pendingBatchChans))
    for i := 0; i < len(pendingBatchChans); i++ {
        pendingBatchChans[i] = make(chan []writeReq, 4)
        freeBatchChans[i] = make(chan []writeReq, 4)
        for j := 0; j < cap(freeBatchChans[i]); j++ {
            freeBatchChans[i] <- make([]writeReq, vs.recoveryBatchSize)
        }
    }
    wg := &sync.WaitGroup{}
    wg.Add(len(pendingBatchChans))
    for i := 0; i < len(pendingBatchChans); i++ {
        go func(pendingBatchChan chan []writeReq, freeBatchChan chan []writeReq) {
            for {
                batch := <-pendingBatchChan
                if batch == nil {
                    break
                }
                for j := 0; j < len(batch); j++ {
                    wr := &batch[j]
                    if wr.timestampbits&_TSB_LOCAL_REMOVAL != 0 {
                        wr.blockID = 0
                    }
                    if vs.logDebug != nil {
                        if vs.vlm.Set(wr.keyA, wr.keyB{{if eq .t "group"}}, wr.nameKeyA, wr.nameKeyB{{end}}, wr.timestampbits, wr.blockID, wr.offset, wr.length, true) < wr.timestampbits {
                            atomic.AddInt64(&causedChangeCount, 1)
                        }
                    } else {
                        vs.vlm.Set(wr.keyA, wr.keyB{{if eq .t "group"}}, wr.nameKeyA, wr.nameKeyB{{end}}, wr.timestampbits, wr.blockID, wr.offset, wr.length, true)
                    }
                }
                freeBatchChan <- batch
            }
            wg.Done()
        }(pendingBatchChans[i], freeBatchChans[i])
    }
    fromDiskBuf := make([]byte, vs.checksumInterval+4)
    fromDiskOverflow := make([]byte, 0, _{{.TT}}_FILE_ENTRY_SIZE)
    batches := make([][]writeReq, len(freeBatchChans))
    batchesPos := make([]int, len(batches))
    fp, err := os.Open(vs.pathtoc)
    if err != nil {
        return err
    }
    names, err := fp.Readdirnames(-1)
    fp.Close()
    if err != nil {
        return err
    }
    sort.Strings(names)
    for i := 0; i < len(names); i++ {
        if !strings.HasSuffix(names[i], ".{{.t}}toc") {
            continue
        }
        namets := int64(0)
        if namets, err = strconv.ParseInt(names[i][:len(names[i])-len(".{{.t}}toc")], 10, 64); err != nil {
            vs.logError("bad timestamp in name: %#v\n", names[i])
            continue
        }
        if namets == 0 {
            vs.logError("bad timestamp in name: %#v\n", names[i])
            continue
        }
        vf, err := new{{.T}}File(vs, namets, osOpenReadSeeker)
        if err != nil {
            vs.logError("error opening %s: %s\n", names[i], err)
            continue
        }
        fp, err := os.Open(path.Join(vs.pathtoc, names[i]))
        if err != nil {
            vs.logError("error opening %s: %s\n", names[i], err)
            continue
        }
        checksumFailures := 0
        first := true
        terminated := false
        fromDiskOverflow = fromDiskOverflow[:0]
        for {
            n, err := io.ReadFull(fp, fromDiskBuf)
            if n < 4 {
                if err != io.EOF && err != io.ErrUnexpectedEOF {
                    vs.logError("error reading %s: %s\n", names[i], err)
                }
                break
            }
            n -= 4
            if murmur3.Sum32(fromDiskBuf[:n]) != binary.BigEndian.Uint32(fromDiskBuf[n:]) {
                checksumFailures++
            } else {
                j := 0
                if first {
                    if !bytes.Equal(fromDiskBuf[:_{{.TT}}_FILE_HEADER_SIZE-4], []byte("{{.TT}}STORETOC v0            ")) {
                        vs.logError("bad header: %s\n", names[i])
                        break
                    }
                    if binary.BigEndian.Uint32(fromDiskBuf[_{{.TT}}_FILE_HEADER_SIZE-4:]) != vs.checksumInterval {
                        vs.logError("bad header checksum interval: %s\n", names[i])
                        break
                    }
                    j += _{{.TT}}_FILE_HEADER_SIZE
                    first = false
                }
                if n < int(vs.checksumInterval) {
                    if binary.BigEndian.Uint32(fromDiskBuf[n-_{{.TT}}_FILE_TRAILER_SIZE:]) != 0 {
                        vs.logError("bad terminator size marker: %s\n", names[i])
                        break
                    }
                    if !bytes.Equal(fromDiskBuf[n-4:n], []byte("TERM")) {
                        vs.logError("bad terminator: %s\n", names[i])
                        break
                    }
                    n -= _{{.TT}}_FILE_TRAILER_SIZE
                    terminated = true
                }
                if len(fromDiskOverflow) > 0 {
                    j += _{{.TT}}_FILE_ENTRY_SIZE - len(fromDiskOverflow)
                    fromDiskOverflow = append(fromDiskOverflow, fromDiskBuf[j-_{{.TT}}_FILE_ENTRY_SIZE+len(fromDiskOverflow):j]...)
                    keyB := binary.BigEndian.Uint64(fromDiskOverflow[8:])
                    k := keyB % workers
                    if batches[k] == nil {
                        batches[k] = <-freeBatchChans[k]
                        batchesPos[k] = 0
                    }
                    wr := &batches[k][batchesPos[k]]
                    {{if eq .t "value"}}
                    wr.keyA = binary.BigEndian.Uint64(fromDiskOverflow)
                    wr.keyB = keyB
                    wr.timestampbits = binary.BigEndian.Uint64(fromDiskOverflow[16:])
                    wr.blockID = vf.id
                    wr.offset = binary.BigEndian.Uint32(fromDiskOverflow[24:])
                    wr.length = binary.BigEndian.Uint32(fromDiskOverflow[28:])
                    {{else}}
                    wr.keyA = binary.BigEndian.Uint64(fromDiskOverflow)
                    wr.keyB = keyB
                    wr.nameKeyA = binary.BigEndian.Uint64(fromDiskOverflow[16:])
                    wr.nameKeyB = binary.BigEndian.Uint64(fromDiskOverflow[24:])
                    wr.timestampbits = binary.BigEndian.Uint64(fromDiskOverflow[32:])
                    wr.blockID = vf.id
                    wr.offset = binary.BigEndian.Uint32(fromDiskOverflow[40:])
                    wr.length = binary.BigEndian.Uint32(fromDiskOverflow[44:])
                    {{end}}
                    batchesPos[k]++
                    if batchesPos[k] >= vs.recoveryBatchSize {
                        pendingBatchChans[k] <- batches[k]
                        batches[k] = nil
                    }
                    fromDiskCount++
                    fromDiskOverflow = fromDiskOverflow[:0]
                }
                for ; j+_{{.TT}}_FILE_ENTRY_SIZE <= n; j += _{{.TT}}_FILE_ENTRY_SIZE {
                    keyB := binary.BigEndian.Uint64(fromDiskBuf[j+8:])
                    k := keyB % workers
                    if batches[k] == nil {
                        batches[k] = <-freeBatchChans[k]
                        batchesPos[k] = 0
                    }
                    wr := &batches[k][batchesPos[k]]
                    {{if eq .t "value"}}
                    wr.keyA = binary.BigEndian.Uint64(fromDiskBuf[j:])
                    wr.keyB = keyB
                    wr.timestampbits = binary.BigEndian.Uint64(fromDiskBuf[j+16:])
                    wr.blockID = vf.id
                    wr.offset = binary.BigEndian.Uint32(fromDiskBuf[j+24:])
                    wr.length = binary.BigEndian.Uint32(fromDiskBuf[j+28:])
                    {{else}}
                    wr.keyA = binary.BigEndian.Uint64(fromDiskBuf[j:])
                    wr.keyB = keyB
                    wr.nameKeyA = binary.BigEndian.Uint64(fromDiskBuf[j+16:])
                    wr.nameKeyB = binary.BigEndian.Uint64(fromDiskBuf[j+24:])
                    wr.timestampbits = binary.BigEndian.Uint64(fromDiskBuf[j+32:])
                    wr.blockID = vf.id
                    wr.offset = binary.BigEndian.Uint32(fromDiskBuf[j+40:])
                    wr.length = binary.BigEndian.Uint32(fromDiskBuf[j+44:])
                    {{end}}
                    batchesPos[k]++
                    if batchesPos[k] >= vs.recoveryBatchSize {
                        pendingBatchChans[k] <- batches[k]
                        batches[k] = nil
                    }
                    fromDiskCount++
                }
                if j != n {
                    fromDiskOverflow = fromDiskOverflow[:n-j]
                    copy(fromDiskOverflow, fromDiskBuf[j:])
                }
            }
            if err != nil && err != io.EOF && err != io.ErrUnexpectedEOF {
                vs.logError("error reading %s: %s\n", names[i], err)
                break
            }
        }
        fp.Close()
        if !terminated {
            vs.logError("early end of file: %s\n", names[i])
        }
        if checksumFailures > 0 {
            vs.logWarning("%d checksum failures for %s\n", checksumFailures, names[i])
        }
    }
    for i := 0; i < len(batches); i++ {
        if batches[i] != nil {
            pendingBatchChans[i] <- batches[i][:batchesPos[i]]
        }
        pendingBatchChans[i] <- nil
    }
    wg.Wait()
    if vs.logDebug != nil {
        dur := time.Now().Sub(start)
        stats := vs.Stats(false).(*{{.T}}StoreStats)
        vs.logInfo("%d key locations loaded in %s, %.0f/s; %d caused change; %d resulting locations referencing %d bytes.\n", fromDiskCount, dur, float64(fromDiskCount)/(float64(dur)/float64(time.Second)), causedChangeCount, stats.Values, stats.ValueBytes)
    }
    return nil
}
